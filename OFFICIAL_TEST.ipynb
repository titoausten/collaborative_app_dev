{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MERGE ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files(directory_path):\n",
    "    # Get all CSV files in the directory\n",
    "    csv_files = list(Path(directory_path).glob('*.csv'))\n",
    "    \n",
    "    # Initialize list to store dataframes\n",
    "    dfs = []\n",
    "    \n",
    "    # Read each CSV file and add filename as a column\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        filename = file.stem  # Get filename without extension\n",
    "        df['Event Code'] = filename\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Filter out \"Booker not confirmed\" rows\n",
    "    merged_df = merged_df[merged_df['Attendee Status'] != 'Booker not attending']\n",
    "    \n",
    "    # Export merged dataframe\n",
    "    output_path = os.path.join(\"/workspaces/collaborative_app_dev/Data/Cleaned Data\", 'merged_data.csv')\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "directory = \"/workspaces/collaborative_app_dev/Data/Raw Data\"\n",
    "output_file = merge_csv_files(directory)\n",
    "\n",
    "df = pd.read_csv(\"/workspaces/collaborative_app_dev/Data/Cleaned Data/merged_data.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GROUP TICKETS BY DATE CREATED BASED ON EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_dates_dict():\n",
    "    # Dictionary mapping event codes to their dates and target audience\n",
    "    return {\n",
    "        'D19': {'date': '2019-11-19', 'audience': 'IT Managers'},\n",
    "        'D21': {'date': '2021-12-09', 'audience': 'IT Managers'},\n",
    "        'D24': {'date': '2024-10-03', 'audience': 'IT Managers'},\n",
    "        'GP21': {'date': '2021-04-22', 'audience': 'Property Managers'},\n",
    "        'GP24': {'date': '2024-09-11', 'audience': 'Property Managers'},\n",
    "        'MSE21': {'date': '2021-03-24', 'audience': 'Education property managers'},\n",
    "        'NP21': {'date': '2021-11-09', 'audience': 'Property Managers'},\n",
    "        'NP24': {'date': '2024-11-06', 'audience': 'Property Managers'},\n",
    "        'SRM22': {'date': '2022-06-15', 'audience': 'Education Managers'},\n",
    "        'SRM23': {'date': '2023-06-08', 'audience': 'Education Managers'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_registrations(merged_csv_path):\n",
    "    # Read the merged CSV\n",
    "    df = pd.read_csv(merged_csv_path)\n",
    "    \n",
    "    # Convert date_created to datetime if it's not already\n",
    "    df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
    "    \n",
    "    # Group by date and event (Event Code) and count registrations\n",
    "    registration_counts = df.groupby([\n",
    "        df['Created Date'].dt.date,\n",
    "        'Event Code'\n",
    "    ]).size().reset_index(name='registration_count')\n",
    "    \n",
    "    # Sort by date and event\n",
    "    registration_counts = registration_counts.sort_values(['Created Date', 'Event Code'])\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "    # Calculate cumulative registrations for each event separately\n",
    "    registration_counts['cumulative_registrations'] = registration_counts.groupby('Event Code')['registration_count'].cumsum()\n",
    "    \n",
    "\n",
    "    # Add event dates and target audience\n",
    "    event_dates = create_event_dates_dict()\n",
    "    \n",
    "    # Add event date and target audience columns\n",
    "    registration_counts['Event date'] = registration_counts['Event Code'].map(\n",
    "        {k: pd.to_datetime(v['date']) for k, v in event_dates.items()}\n",
    "    )\n",
    "\n",
    "    # Remove registrations after event date\n",
    "    registration_counts = registration_counts[registration_counts['Created Date'] <= registration_counts['Event date']]\n",
    "    \n",
    "    registration_counts['Target audience'] = registration_counts['Event Code'].map(\n",
    "        {k: v['audience'] for k, v in event_dates.items()}\n",
    "    )\n",
    "\n",
    "    # Calculate days until event\n",
    "    registration_counts['Days until event'] = (\n",
    "        registration_counts['Event date'] - pd.to_datetime(registration_counts['Created Date'])\n",
    "    ).dt.days\n",
    "\n",
    "\n",
    "    # Sort by date and event for final output\n",
    "    registration_counts = registration_counts.sort_values(['Created Date', 'Event Code'])\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # Detect promotional spikes\n",
    "    mean_daily = registration_counts['registration_count'].mean()\n",
    "    std_daily = registration_counts['registration_count'].std()\n",
    "    registration_counts['promotional_spike'] = (registration_counts['registration_count'] > \n",
    "                                        (mean_daily + 2 * std_daily)).astype(int)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # Export the analysis\n",
    "    analysis_path = os.path.join(os.path.dirname(\"/workspaces/collaborative_app_dev/Data/Cleaned Data/\"), 'complete_registration_analysis.csv')\n",
    "    registration_counts.to_csv(analysis_path, index=False)\n",
    "    \n",
    "    return analysis_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = analyze_registrations(\"/workspaces/collaborative_app_dev/Data/Cleaned Data/merged_data.csv\")\n",
    "\n",
    "df = pd.read_csv(analysis_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING AND EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/workspaces/collaborative_app_dev/Data/Cleaned Data/complete_registration_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Event date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"Extract features with enhanced pattern detection\"\"\"\n",
    "    # days = week_number * 7 #SHOULD BE EVENT DATE - CURRENT DATE\n",
    "\n",
    "    # Calculate days until event\n",
    "    days = (df['Event date'] - pd.to_datetime(datetime.now())).dt.days\n",
    "\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for event_code in df['Event Code'].unique():\n",
    "        event_df = df[df['Event Code'] == event_code].copy()\n",
    "        #event_df['days_from_start'] = (event_df['date'] - event_df['date'].min()).dt.days\n",
    "        \n",
    "        if event_df['Days until event'].max() < days:\n",
    "            continue\n",
    "        \n",
    "        #### LOOK AT THIS!!!!\n",
    "        registrations_at_point = event_df[event_df['Days until event'] >= days]\n",
    "        \n",
    "        if len(registrations_at_point) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Enhanced feature extraction\n",
    "        recent_registrations = registrations_at_point.tail(7)['registration_count'].mean()\n",
    "        early_registrations = registrations_at_point.head(7)['registration_count'].mean()\n",
    "\n",
    "        #--------------------------------------------------------------------------------------------\n",
    "        # Calculate days since last spike\n",
    "        spike_mask = registrations_at_point['promotional_spike'] == 1\n",
    "        if any(spike_mask):\n",
    "            # Get the most recent spike's days until event\n",
    "            last_spike_days = registrations_at_point[spike_mask]['Days until event'].max()\n",
    "            # Calculate the difference between current point and last spike\n",
    "            days_since_spike = registrations_at_point['Days until event'].max() - last_spike_days\n",
    "        else:\n",
    "            # If no spikes, use 0 to indicate no prior spikes have occurred\n",
    "            days_since_spike = 0\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        feature_dict = {\n",
    "            'current_registrations': registrations_at_point['cumulative_registrations'].max(),\n",
    "            'avg_daily_rate': registrations_at_point['registration_count'].mean(),\n",
    "            'recent_velocity': recent_registrations,\n",
    "            'early_velocity': early_registrations,\n",
    "            'registration_acceleration': (recent_registrations - early_registrations) / 7,\n",
    "            'days_active': len(registrations_at_point),\n",
    "            'peak_daily_registrations': registrations_at_point['registration_count'].max(),\n",
    "            'registration_volatility': registrations_at_point['registration_count'].std(),\n",
    "            #-----------------------------------------------------------------------------\n",
    "            'spike_count': registrations_at_point['promotional_spike'].sum(),\n",
    "            'days_since_last_spike': days_since_spike,\n",
    "            'event_start_date': registrations_at_point['Event date'],\n",
    "            'target_audience': registrations_at_point['Target audience'],\n",
    "            'event_code': registrations_at_point['Event Code'],\n",
    "            }\n",
    "\n",
    "\n",
    "        features.append(feature_dict)\n",
    "        targets.append(event_df['cumulative_registrations'].max())\n",
    "\n",
    "    return pd.DataFrame(features), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6722/2065288806.py:6: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized.\n",
      "  days = (df['Event date'] - df['Created Date']).dt.days\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m current_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoday\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract features with enhanced pattern detection\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# days = week_number * 7 #SHOULD BE EVENT DATE - CURRENT DATE\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate days until event\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m days \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEvent date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCreated Date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays\n\u001b[1;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:273\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    267\u001b[0m     should_extension_dispatch(left, right)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1454\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_timedelta_arraylike(\u001b[38;5;241m-\u001b[39mother)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(other_dtype):\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;66;03m# e.g. Array/Index of DateOffset objects\u001b[39;00m\n\u001b[0;32m-> 1454\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_addsub_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(other_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1456\u001b[0m     other_dtype, DatetimeTZDtype\n\u001b[1;32m   1457\u001b[0m ):\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# DatetimeIndex, ndarray[datetime64]\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sub_datetime_arraylike(other)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1350\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._addsub_object_array\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;66;03m# Caller is responsible for broadcasting if necessary\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39mshape, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, other\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m-> 1350\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "current_date = pd.to_datetime('today')\n",
    "\n",
    "features, targets = extract_features(df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
